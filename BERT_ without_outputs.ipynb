{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1pBL6FqLtr63PJzSifnFKZs8WD5RnTZLy",
      "authorship_tag": "ABX9TyO8HWdAVkBE9uVOTqDp0acu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshsurya59/Offensive-Language-Detection-BERT/blob/main/BERT_%20without_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.28.0"
      ],
      "metadata": {
        "id": "z57Q2pUQCwEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive - Only for Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "70YHyrTRSsEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Loading required libraries ============================#\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#!pip install datasets\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer,TFAutoModelForSequenceClassification\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "U2xaltPqSsQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Loading the dataset ============================#\n",
        "\n",
        "#df = pd.read_csv('/content/drive/MyDrive/hate_offensive_data.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/text/hate_offensive_data.csv')\n",
        "del df['Unnamed: 0']\n",
        "\n",
        "df['label'] = np.where(df['class']==2, 0, 1)\n",
        "\n",
        "# Class 0 - Appropriate (Safe)\n",
        "# Class 1 - Inappropriate (Hateful or offensive)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "BlNM0tn6TJ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Text Preprocessing ============================#\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add(\"rt\") # adding rt to remove retweet in dataset\n",
        "\n",
        "# Removing Emojis\n",
        "def strip_entities(raw_text):\n",
        "    entity_regex = r\"&[^\\s;]+;\"\n",
        "    text = re.sub(entity_regex, \"\", raw_text)\n",
        "    return text\n",
        "\n",
        "# Replacing user tags\n",
        "def remove_mentions(raw_text):\n",
        "    regex = r\"@([^ ]+)\"\n",
        "    text = re.sub(regex, \"\", raw_text)\n",
        "    return text\n",
        "\n",
        "# Removing URLs\n",
        "def remove_urls(raw_text):\n",
        "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))\"\n",
        "    text = re.sub(url_regex, '', raw_text)\n",
        "    return text\n",
        "\n",
        "# Removing Unnecessary Symbols\n",
        "def clean_symbols(raw_text):\n",
        "    text = raw_text.replace('\"', '')\n",
        "    text = text.replace(\"'\", '')\n",
        "    text = text.replace(\"!\", '')\n",
        "    text = text.replace(\"`\", '')\n",
        "    text = text.replace(\"..\", '')\n",
        "    text = text.replace(\".\", '')\n",
        "    text = text.replace(\",\", '')\n",
        "    text = text.replace(\"#\", '')\n",
        "    text = text.replace(\":\", '')\n",
        "    text = text.replace(\"?\", '')\n",
        "    return text\n",
        "\n",
        "# Stemming\n",
        "def stemming(raw_text):\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in raw_text.split()]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Removing stopwords\n",
        "def filter_stopwords(raw_text):\n",
        "    tokenize = word_tokenize(raw_text)\n",
        "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "def preprocess(data):\n",
        "    clean = []\n",
        "    clean = [text.lower() for text in data]\n",
        "    clean = [remove_mentions(text) for text in clean]\n",
        "    clean = [strip_entities(text) for text in clean]\n",
        "    clean = [remove_urls(text) for text in clean]\n",
        "    clean = [clean_symbols(text) for text in clean]\n",
        "    clean = [stemming(text) for text in clean]\n",
        "    clean = [filter_stopwords(text) for text in clean]\n",
        "\n",
        "    return clean\n",
        "\n",
        "nltk.download('punkt_tab') # Download the missing NLTK resource\n",
        "\n",
        "tweets = list(df['tweet'])\n",
        "labels = list(df['label'])\n",
        "clean_tweets = preprocess(tweets)\n",
        "\n",
        "df_tweet = pd.DataFrame({'tweet': tweets, 'clean_tweet': clean_tweets, 'label': labels})"
      ],
      "metadata": {
        "id": "NjnVk-EUTTsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Read Preprocessed Data ============================#\n",
        "\n",
        "#df = pd.read_csv('/content/drive/MyDrive/preprocessed_data.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/text/preprocessed_data.csv')\n",
        "df['clean_tweet'] = df['clean_tweet'].astype(str)\n",
        "df['tweet'] = df['tweet'].astype(str)\n",
        "\n",
        "tweets = list(df['tweet'])\n",
        "clean_tweets = list(df['clean_tweet'])\n",
        "labels = list(df['label'])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "_grspdu8SyOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweet"
      ],
      "metadata": {
        "id": "0k2LKjbpSyLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweet['label'].value_counts()\n",
        "# Class 0 - Appropriate (Safe)\n",
        "# Class 1 - Inappropriate (Hateful or offensive)"
      ],
      "metadata": {
        "id": "AflR2DSkSyIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_distribution = df_tweet['label'].value_counts()\n",
        "plt.figure(figsize=(8, 6))\n",
        "class_distribution.plot(kind='bar', color=['#4CAF50', '#F44336'], edgecolor='black')\n",
        "plt.title('Class Distribution', fontsize=16)\n",
        "plt.xlabel('Label', fontsize=14)\n",
        "plt.ylabel('No of Tweets', fontsize=14)\n",
        "plt.xticks(rotation=0, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add grid lines to y-axis with dashed style\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XizULafQSyFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Tokenization ============================#\n",
        "\n",
        "# Split into training, validation and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(clean_tweets, labels, test_size=0.2, random_state=42)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Loading pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "AY75i7c9SyCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Defining a PyTorch Dataset ============================#\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)\n",
        "test_dataset = CustomDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "PMpKsfGRSx_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Loading the pre-trained Model ============================#\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Defining optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Defining data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "MpLfU6VISx8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    # Wrap the train_loader with tqdm for a progress bar\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_accuracy = 0\n",
        "    # Wrap the val_loader with tqdm for a progress bar\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Validating Epoch {epoch + 1}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            val_accuracy += torch.sum(predictions == labels).item() / labels.size(0)\n",
        "\n",
        "    avg_val_accuracy = val_accuracy / len(val_loader)\n",
        "    print(f'Epoch {epoch + 1}, Validation Accuracy: {avg_val_accuracy}')"
      ],
      "metadata": {
        "id": "HfWTJ2NhSxxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Evaluating Model Performance ============================#\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Ensure model is on the correct device before evaluation\n",
        "model.to(device)\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predicted_labels.extend(predictions.cpu().numpy())"
      ],
      "metadata": {
        "id": "jzdnYr-WUN92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={\"size\": 14})\n",
        "plt.title('Confusion Matrix - BERT', fontsize = 16)\n",
        "plt.xlabel('Predicted', fontsize = 14)\n",
        "plt.ylabel('Actual', fontsize = 14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Classification report\n",
        "target_names = ['Appropriate', 'Inappropriate']\n",
        "print('\\nClassification Report:\\n\\n', classification_report(true_labels, predicted_labels, target_names=target_names))\n",
        "\n",
        "# Evaluation Metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "print(\"\\nEvaluation Metrics:\\n\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "id": "PZBcV4uIUN6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Save the  model ============================#\n",
        "\n",
        "# model.save_pretrained('/content/drive/MyDrive/text/bert_model/')\n",
        "# model.save_pretrained('/bert_model/')"
      ],
      "metadata": {
        "id": "ztKHCF_PUN0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================ Prediction ============================#\n",
        "\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "#Text Preprocessing\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add(\"rt\") # adding rt to remove retweet in dataset\n",
        "\n",
        "# Removing Emojis\n",
        "def remove_entity(raw_text):\n",
        "    entity_regex = r\"&[^\\s;]+;\"\n",
        "    text = re.sub(entity_regex, \"\", raw_text)\n",
        "    return text\n",
        "\n",
        "# Replacing user tags\n",
        "def change_user(raw_text):\n",
        "    regex = r\"@([^ ]+)\"\n",
        "    text = re.sub(regex, \"\", raw_text)\n",
        "    return text\n",
        "\n",
        "# Removing URLs\n",
        "def remove_url(raw_text):\n",
        "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))\"\n",
        "    text = re.sub(url_regex, '', raw_text)\n",
        "    return text\n",
        "\n",
        "# Removing Unnecessary Symbols\n",
        "def remove_noise_symbols(raw_text):\n",
        "    text = raw_text.replace('\"', '')\n",
        "    text = text.replace(\"'\", '')\n",
        "    text = text.replace(\"!\", '')\n",
        "    text = text.replace(\"`\", '')\n",
        "    text = text.replace(\"..\", '')\n",
        "    text = text.replace(\".\", '')\n",
        "    text = text.replace(\",\", '')\n",
        "    text = text.replace(\"#\", '')\n",
        "    text = text.replace(\":\", '')\n",
        "    text = text.replace(\"?\", '')\n",
        "    return text\n",
        "\n",
        "# Stemming\n",
        "def stemming(raw_text):\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in raw_text.split()]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Removing stopwords\n",
        "def remove_stopwords(raw_text):\n",
        "    tokenize = word_tokenize(raw_text)\n",
        "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "def preprocess(data):\n",
        "    clean = []\n",
        "    clean = [text.lower() for text in data]\n",
        "    clean = [change_user(text) for text in clean]\n",
        "    clean = [remove_entity(text) for text in clean]\n",
        "    clean = [remove_url(text) for text in clean]\n",
        "    clean = [remove_noise_symbols(text) for text in clean]\n",
        "    clean = [stemming(text) for text in clean]\n",
        "    clean = [remove_stopwords(text) for text in clean]\n",
        "\n",
        "    return clean"
      ],
      "metadata": {
        "id": "HuNDjCAGVUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QgYH9rsVUfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4D5-ItkVUUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00c5daef"
      },
      "source": [
        "#============================ Load the saved model ============================#\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "loaded_model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/text/bert_model', local_files_only=True)\n",
        "\n",
        "# You can now use loaded_model for predictions or further evaluation\n",
        "print(\"Model loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/text/bert_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Your input\n",
        "# custom_input_text = \" now you're ready to vote for donald trump \"\n",
        "\n",
        "custom_input_text = \"why the f**k\"\n",
        "\n",
        "# Optional: dummy preprocess function (replace if you have a custom one)\n",
        "def preprocess(text_list):\n",
        "    return text_list\n",
        "\n",
        "custom_text_list = [custom_input_text]\n",
        "preprocessed_custom_text = preprocess(custom_text_list)[0]\n",
        "\n",
        "# Tokenization\n",
        "tokenized_input = tokenizer(preprocessed_custom_text, return_tensors='pt')\n",
        "\n",
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(**tokenized_input)\n",
        "    logits = output.logits\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "predicted_label = torch.argmax(probabilities, dim=1).item()\n",
        "probability_class_0 = probabilities[0][0].item()\n",
        "probability_class_1 = probabilities[0][1].item()\n",
        "threshold = 0.4\n",
        "\n",
        "if probability_class_1 >= threshold:\n",
        "    print(\"‚ö†Ô∏è Prediction: Inappropriate (Offensive)\")\n",
        "else:\n",
        "    print(\"‚úÖ Prediction: Appropriate (Safe)\")\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"Predicted Label:\", predicted_label)\n",
        "print(\"Probability for Class 0 (Appropriate):\", probability_class_0)\n",
        "print(\"Probability for Class 1 (Inappropriate):\", probability_class_1)\n",
        "\n",
        "# ========================= üìä Show Probability Bar Chart ========================= #\n",
        "classes = ['Appropriate (Safe)', 'Inappropriate (Offensive)']\n",
        "probs = [probability_class_0, probability_class_1]\n",
        "\n",
        "plt.figure(figsize=(8, 2))\n",
        "bars = plt.barh(classes, probs, color=['green', 'red'])\n",
        "plt.xlim(0, 1)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.title(f\"Prediction: {'Safe' if predicted_label == 0 else 'Offensive'}\")\n",
        "\n",
        "# Decide final prediction label based on threshold\n",
        "threshold = 0.4\n",
        "final_prediction = \"Offensive\" if probability_class_1 >= threshold else \"Safe\"\n",
        "\n",
        "plt.title(f\"Prediction: {final_prediction}\")\n",
        "\n",
        "\n",
        "# Add text on bars\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,\n",
        "             f\"{bar.get_width()*100:.2f}%\", va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3xk3RmfQSUX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdriNSVDbWlP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}